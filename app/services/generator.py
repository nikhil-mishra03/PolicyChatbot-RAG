# accepts relevant chunks from the user
# returns a response generated by the LLM based on the context from the vector storage
from typing import Any
from app.core.config import Settings
from openai import AsyncOpenAI
from openai import OpenAIError as openai_error
from app.core.logger_config import get_logger

_openai_client_instance = None

logger = get_logger(__name__)   

class GeneratorService:
    def __init__(self, settings: Settings):
        self.settings = settings
        self.logger = logger

    def _get_openai_client(self) -> AsyncOpenAI:
        global _openai_client_instance
        if _openai_client_instance is not None:
            return _openai_client_instance
        _openai_client_instance = AsyncOpenAI(api_key=self.settings.openai_api_key)
        return _openai_client_instance


    async def generate_answer(self, *, question: str, retrieval_results: list[dict]):
        if not retrieval_results:
            raise RuntimeError("No relevant documents found for the given question.")
            
        context_blocks = []
        for i, res in enumerate(retrieval_results):
            meta = res.get("metadata", {})
            text = res.get("text", "")
            score = res.get("score", 0.0)
            context_blocks.append(
                f"source {i+1} ({meta.get('s3_key', 'unknown')} | chunk {meta.get('chunk_index', '?')}): {text} (score: {score:.4f})"
            )

        context = "\n\n".join(context_blocks)
        prompt = (
            "You are a compliance assistant. Answer the question strictly using the provided policy context.\n"
            "If the answer is not in the context, say you do not know.\n\n"
            f"Context:\n{context}\n\nQuestion: {question}\nAnswer:"
        )
        try:
            client = self._get_openai_client()
            response = await client.chat.completions.create(
                model = self.settings.openai_model_name,
                messages=[
                    {"role": "system", "content": "You are a helpful policy assistant..."},
                    {"role": "user", "content": prompt},
                ],
                # temperature=0.0, # Lower temperature for strictly factual answers
            )
        except openai_error as e:
            self.logger.error(f"OpenAI API error: {e}")
            raise RuntimeError(f"OpenAI API error: {e}")
        
        self.logger.info("Generated response from OpenAI successfully.")
        answer = response.choices[0].message.content.strip()
        
        sources = [
            {
                "chunk_id": str(res["metadata"].get("chunk_index", "unknown")),
                "tenant_id": str(res["metadata"].get("tenant_id", "unknown")),
                "s3_key": str(res["metadata"].get("s3_key", "unknown")),
                "distance": float(res.get("score", 0.0)) # Mapping score to distance field in schema
            }
            for res in retrieval_results
        ]

        return {
            "answer": answer,
            "sources": sources
        }

        


    
        


